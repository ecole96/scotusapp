# this file contains functions universal to the project, used in every module
import datetime
import tldextract
import re
import html
import requests
import math
import os
import smtplib
import pickle
import zipfile
import glob
from bs4 import BeautifulSoup
from urllib import parse as urlparse
from sklearn.svm import LinearSVC
from sklearn.calibration import CalibratedClassifierCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
from scipy.sparse import hstack
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive

# download a webpage using BeautifulSoup
# returns soup object we can parse
def downloadPage(url):
    #user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/602.2.14 (KHTML, like Gecko) Version/10.0.1 Safari/602.2.14'
    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36'
    try:
        request = requests.get(url,headers={'User-Agent':user_agent})
        page = request.content
        soup = BeautifulSoup(page, 'html.parser')
    except Exception as e: # couldn't download page
        print("DOWNLOAD ERROR: ",e)
        soup = None
    return soup

# alternate webpage download function for sites that need Selenium for scraping
def alt_downloadPage(driver,url,waitElement):
    try:
        driver.get(url)
        if waitElement: #waitElement is an element on a seleniumSource webpage that we use to confirm a site loaded successfully
            wait = WebDriverWait(driver,10)
            element_present = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR,waitElement)))
        page = driver.page_source
        soup = BeautifulSoup(page, 'lxml')
    except Exception as e:
        print("DOWNLOAD ERROR (Selenium):",e)
        soup = None
    return soup

# determines whether to use Selenium or the standard requests library for downloading article webpages, and handles webdriver initializations as they are needed
# returns a 'driver' - if we need Selenium, a Chrome webdriver is returned. Otherwise, return None (and just use our normal download method)
def decideScraperMethod(source,si):
    seleniumSources = ["wsj"]
    driver = None
    if source in seleniumSources: # an article we want is from a page we need Selenium to scrape
        if not si.attemptedDriver: # Chrome webdriver hasn't been opened yet, so do that
            print("Initializing Selenium...")
            si.initializeDriver()
        if si.driver and not si.isLoggedIn(source) and not si.hasAttemptedLogin(source): # if the webdriver opened successfully and we need to login, do that
            print("Logging into ",source+"...")
            si.login(source)
        if si.driver and si.isLoggedIn(source): # if we already have a webdriver and we're already logged into our article source, then use the webdriver
            driver = si.driver
    return driver

# Links in the RSS feed are generated by google, and the actual URL of the article is stored as the 'url' parameter in this link
# this function gets us the actual URL
def getURL(RSS_url):
    parsed = urlparse.urlparse(RSS_url)
    url = urlparse.parse_qs(parsed.query)['url'][0]
    return url

# original date is a long date/time string, raw_format is the format that the date is initially entered as
# for our purposes, we really only need date, not time - so this function extracts the date and converts it into a yyyy-mm-dd format (how MySQL stores dates)
def convertDate(orig_date,raw_format):
    convertedDate = datetime.datetime.strptime(orig_date,raw_format).strftime('%Y-%m-%d %H:%M:%S')
    return convertedDate

# converts a 12 hour time to 24 hour time for storage in the database
def convertTime(timestr,raw_format):
    std = datetime.datetime.strptime(timestr, raw_format)
    return datetime.datetime.strftime(std, "%H:%M:00")

# parses URL to get the domain name of each article's link - the source
# one defect in handling the source is that, as of now, we don't know how to handle multiple-word sources beyond just storing it all as one string (so Fox News would just be stored as foxnews)
def getSource(url):
    ext = tldextract.extract(url.lower())
    source = ext.domain
    subdomain = ext.subdomain.replace('www','')
    if source in ['iheart','radio','cbslocal','go'] or (source == 'timesofisrael' and subdomain and subdomain != 'blogs'): # exceptions with subdomains
        source = '.'.join([subdomain,source,ext.suffix])
        if source.startswith("."): # iheart and radio links sometimes have no station subdomain, so just revert back to primary domain
            source = ext.domain
    elif source in ['spectator','abc','standard','thestar','inquirer','thegazette']: # exceptions that need the suffixes
        source = '.'.join([source,ext.suffix])
    return source

# 'Supreme Court' appears in the titles of the RSS feed with bold tags around them
# this function strips the HTML and returns a text-only version of the title
def cleanTitle(original_title):
    cleanr = re.compile('<.*?>')
    cleanTitle = html.unescape(re.sub(cleanr, '', original_title))
    return cleanTitle

# for use with generic scraper - takes out known 'fluff' (like advertisements and prompts to read more), attempts to strip text to the essentials
def cleanText(text):
    cleanedText = ''
    for line in text.split('\n'):
        line = line.strip()
        if line.lower() not in ["ad","advertisement","story continued below",'']:
            cleanedText += (line + '\n\n')
    return cleanedText.strip()

# print preliminary article information
def printBasicInfo(title,url):
    print('Title:',title)
    print('URL:', url)

# checks whether an article is from a known "bad" source - usually aggregate sites, paywalled sites, or obscure sites that don't scrape well and aren't worth writing a scraper for
def isBlockedSource(url):
    blockedSources = ['law360','law','freerepublic','bloomberglaw','nakedcapitalism','independent','mentalfloss','columbustelegram'] 
    if "howappealing.abovethelaw.com" in url or getSource(url) in blockedSources:
        print("Rejected - URL/source known to have a paywall, or does not contain full articles")
        return True
    else:
        return False

# we're getting duplicate articles due to tiny differences in title text (different types of quotation marks, unicode spaces), so we're normalizing them for comparison
def processTitle(title):
    title = title
    chars = {"’":"'","‘":"'","\xa0":" "}
    for c in chars:
        title = title.replace(c,chars[c])
    return title

# checks whether the title of an article is already in the database, avoiding duplicates
# we only check for title and url because the likeliness of identical titles is crazy low, and it cuts down on reposts from other sites
def articleIsDuplicate(title,url,c):
    c.execute("""SELECT idArticle FROM article WHERE title = %s OR url = %s""",(title,url,)) # funky single quotes can sometimes lead to duplicates
    if c.rowcount == 0:
        return False
    else:
        print("Rejected - article already exists in the database")
        return True

# check whether an irrelevant article is already in the training data
def rejectedIsDuplicate(title,url,c):
    c.execute("""SELECT id FROM rejectedTrainingData WHERE title = %s OR url = %s""",(title,url,))
    if c.rowcount == 0:
        return False
    else:
        print("Rejected article is already in training data")
        return True

# determines if a new billing cycle for the Google Cloud API has been reached
def isNewBillingCycle(c):
    now = datetime.datetime.now().date()
    newBillingDate = c.execute("""SELECT newBillingDate FROM analysisCap""")
    row = c.fetchone()
    newBillingDate = datetime.datetime.strptime(row['newBillingDate'].strftime("%Y-%m-%d"),"%Y-%m-%d").date()
    if now >= newBillingDate: # check if billing date has been passed
        return True
    else:
        return False

# resets analysisCap table in database for new month of API requests
def resetRequests(c):
    now = datetime.date.today()
    newBillingDate = (now + datetime.timedelta(days=32)).replace(day=1) # reset to the first of next month
    c.execute("UPDATE analysisCap SET newBillingDate=(%s),currentSentimentRequests=0,currentImageRequests=0",(newBillingDate,))

# in the small chance an article doesn't have a title (it does happen, rarely), title it Untitled [date] [time].
def untitledArticle():
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    return "Untitled " + now

# The Google Alerts RSS feeds sometimes truncate long titles with a "..." - this function gets the full title by comparing the feed title against the scraped title
# worst-case scenario, just use the original title
def replaceTitle(originalTitle, scrapedTitle):
    split_title = originalTitle.split()
    title_no_ellipsis = ' '.join(split_title[:-1])
    if title_no_ellipsis.lower() in scrapedTitle.lower():
        print("Truncated title changed to -",scrapedTitle)
        return scrapedTitle
    else:
        return originalTitle

# create training dataset for relevancy check using a LinearSVC model
# return vectorizer and clf (classifier) because we need them to predict relevancy for individual articles later on
# our model will consist of two separate tf-idf matrices (one for article text, another for titles) combined into one
def train_relevancy(c):
    modelFailed = False
    fullpath = "/var/www/html/scotusapp/articleCollector/relevancy_model.pkl"
    if os.path.exists(fullpath): # attempt to load relevancy model (vectorizers and classifiers)
        with open(fullpath,"rb") as f:
            try:
                v_text,v_title,clf = pickle.load(f)
                print("Relevancy model loaded.\n")
            except Exception as e:
                print("ERROR: Failed to open classifier file - ",e)
                modelFailed = True
    else:
        modelFailed = True
    if modelFailed: # generate relevancy model
        print("Training relevancy check model...")
        Xraw, Y = get_training_data(c,True)
        v_text = TfidfVectorizer(stop_words=stopwords.words("english"),min_df=5)
        v_title = TfidfVectorizer(stop_words=stopwords.words("english"),ngram_range=(1,3))
        X = convertTextData(Xraw,v_text,v_title,'train')
        clf = CalibratedClassifierCV(LinearSVC(class_weight='balanced'),method='sigmoid',cv=5).fit(X,Y) #LinearSVC() doesn't have probability functionality by default so wrapping it into CalibratedClassiferCV()
        with open(fullpath, 'wb') as f:
            pickle.dump((v_text,v_title,clf),f) 
    return clf, v_text, v_title

# returns CountVectorizer necessary for determining article similarity
def generate_similarity_model():
    return CountVectorizer(stop_words=stopwords.words("english"))

# training data consists of input (x) and output (y)
# x = 2d array of [article title, article_text]
# y = classification labels ('R' = relevant, 'U' = unrelated topics, 'F' = foreign courts, 'S' = state/local courts)
# if True, binary parameter splits labels only into 'R' and 'U' ('F' and 'S' folded into the latter)
def get_training_data(c,binary):
    x = []
    y = []
    # get relevant training data - label R for relevant
    c.execute("""SELECT article_text,title FROM article WHERE idArticle <= 30604""") # only using training articles I've tested for now (so up to a certain id)
    rows = c.fetchall()
    for r in rows:
        x.append([r["title"],r["article_text"]])
        y.append("R")
    # get irrelevant training data - label U for irrelevant
    c.execute("""SELECT code,text,title FROM rejectedTrainingData WHERE id <= 26301""")
    rows = c.fetchall()
    for r in rows:
        x.append([r["title"],r["text"]])
        code = r["code"]
        if binary and code in ['S','F']:
            code = "U"
        y.append(code)
    return x, y

# in order to use the classifier we have to convert text data (our articles and titles) into numerical data (tf-idf matrices)
# 'mode' parameter determines how to feed the data to the tf-idf vectorizers - if 'train', the data is used to train/fit it. Otherwise, only used to test it/predict.
# returns a combined tf-idf matrix we can use to train our classifier
def convertTextData(x,v_text,v_title,mode):
    Xtitle = []
    Xtext = []
    for row in x:
        Xtitle.append(row[0])
        Xtext.append(row[1])
    if mode == 'train':
        Xtitle = v_title.fit_transform(Xtitle)
        Xtext = v_text.fit_transform(Xtext)
    else:
        Xtitle = v_title.transform(Xtitle)
        Xtext = v_text.transform(Xtext)
    x = hstack([Xtext,Xtitle]) # merge text and title matrices
    return x

# universal function for sending an alert email (used in ScraperAlert and TopicSiteAlert functions)
def sendAlert(subject,text):
    admins = get_admins()
    try:
        print("Sending alert email...")
        server = smtplib.SMTP('smtp.gmail.com', 587)
        server.starttls()
        server.login(os.environ['APP_EMAIL'], os.environ['EMAIL_PASSWORD'])
        toaddr = admins[0]
        cc = []
        if len(admins) > 1:
            cc = [admin for admin in admins[1:]]
        fromaddr = os.environ['APP_EMAIL']
        message = "From: %s\r\n" % fromaddr + "To: %s\r\n" % toaddr + "CC: %s\r\n" % ",".join(cc) + "Subject: %s\r\n" % subject + "\r\n" + text
        toaddrs = [toaddr] + cc
        server.sendmail(fromaddr, toaddrs, message)
        server.quit()
    except SMTPException as e:
        print("Alert email failed to send:",e)

# parses ADMINS environmental variable into a list of emails (used in email alerts)
def get_admins():
    admin_str = os.environ['ADMINS']
    admin_emails = [a for a in admin_str.split(',')]
    return admin_emails

# generates a .zip of the full dataset at the end of the script to allow direct serving of that set (generating it on the fly is quite slow)
def generate_full_dl(c):
    try:
        webapp_path = "/var/www/html/scotusapp/webapp/"
        txtpath = "/var/www/html/scotusapp/txtfiles/"
        [os.remove(f) for f in glob.glob(webapp_path+"Full_Dataset.*.zip")+glob.glob(webapp_path+"Full_Dataset.*.csv")] # remove old ZIP and CSV
        now = datetime.datetime.now()
        filename = "Full_Dataset." + now.strftime("%Y_%m_%d.%H_%M_%S") # generate new file names (CSV and .zip share one minus the extension)
        print("Generating updated zip for full dataset...")
        sql = full_dataset_sql(filename)
        c.execute(sql)
        prepare_csv(webapp_path+filename+".csv")
        with zipfile.ZipFile(webapp_path+filename+".zip",'w',compression=zipfile.ZIP_DEFLATED) as zipf: # generate zip
            zipf.write(webapp_path+filename+".csv",filename+".csv") # inserting data CSV into zip
            [zipf.write(f,os.path.basename(f)) for f in glob.glob(txtpath+"*.txt")]
        print("Done")
    except Exception as e:
        print("Failed to generate full dataset zip:",e)

#prepends headers and byte order mark to CSV
def prepare_csv(path):
    headers = ['"Article ID"', '"Alt ID"', '"Date/Time"', '"Source"', '"MBFC Bias"', '"MBFC Score"', '"MBFC Z-Score"', '"MBFC Factual Reporting"', '"AllSides Bias"', '"AllSides Score"', '"AllSides Z-Score"', '"AllSides Confidence"', 
                    '"AllSides Agreement"', '"AllSides Disagreement"', '"MBM Score"', '"MBM Z-Score"', '"URL"', '"Title"', '"Author"', '"Relevancy Score"', '"Sentiment Score"', '"Sentiment Magnitude"', '"Top Image Entity"', 
                    '"Entity Score"', '"Keywords"', '"Similar Articles - Before Publication"', '"Similar Articles - After Publication"','"FB Reactions - Initial Entry"', '"FB Reactions - Day 1"', '"FB Reactions - Day 7"', '"FB Comments - Initial Entry"', 
                    '"FB Comments - Day 1"', '"FB Comments - Day 7"', '"FB Shares - Initial Entry"', '"FB Shares - Day 1"', '"FB Shares - Day 7"', '"FB Comment Plugin - Initial Entry"', 
                    '"FB Comment Plugin - Day 1"', '"FB Comment Plugin - Day 7"', '"TW Tweets - Initial Entry"', '"TW Tweets - Day 1"', '"TW Tweets - Day 7"', '"TW Total Favorites - Initial Entry"', 
                    '"TW Total Favorites - Day 1"', '"TW Total Favorites - Day 7"', '"TW Total Retweets - Initial Entry"', '"TW Total Retweets - Day 1"', '"TW Total Retweets - Day 7"', 
                    '"TW Top Favorites - Initial Entry"', '"TW Top Favorites - Day 1"', '"TW Top Favorites - Day 7"', '"TW Top Retweets - Initial Entry"', '"TW Top Retweets - Day 1"', 
                    '"TW Top Retweets - Day 7"', '"RDT Posts - Initial Entry"', '"RDT Posts - Day 1"', '"RDT Posts - Day 7"', '"RDT Total Comments - Initial Entry"', '"RDT Total Comments - Day 1"', 
                    '"RDT Total Comments - Day 7"', '"RDT Total Scores - Initial Entry"', '"RDT Total Scores - Day 1"', '"RDT Total Scores - Day 7"', '"RDT Top Comments - Initial Entry"', 
                    '"RDT Top Comments - Day 1"', '"RDT Top Comments - Day 7"', '"RDT Top Score - Initial Entry"', '"RDT Top Score - Day 1"', '"RDT Top Score - Day 7"', '"RDT Top Ratio - Initial Entry"', 
                    '"RDT Top Ratio - Day 1"', '"RDT Top Ratio - Day 7"', '"RDT Average Ratio - Initial Entry"', '"RDT Average Ratio - Day 1"', '"RDT Average Ratio - Day 7"', 
                    '"MBM Political Alignment - Very Conservative"', '"MBM Political Alignment - Very Liberal"', '"MBM Political Alignment - Moderate"', '"MBM Political Alignment - Liberal"', '"MBM Political Alignment - Conservative"', 
                    '"MBM Political Engagement - Moderate"', '"MBM Political Engagement - Liberal"', '"MBM Political Engagement - Conservative"', '"MBM Age - 25-34"', '"MBM Age - 35-44"', '"MBM Age - 45-54"', '"MBM Age - Under 18"', 
                    '"MBM Age - Above 65"', '"MBM Age - 55-64"', '"MBM Age - 18-24"', '"MBM Income ($) - 250k to 350k"', '"MBM Income ($) - 75k to 100k"', '"MBM Income ($) - Over 500k"', '"MBM Income ($) - 125k to 150k"', '"MBM Income ($) - 40k to 50k"',
                    '"MBM Income ($) - 150k to 250k"', '"MBM Income ($) - 100k to 125k"', '"MBM Income ($) - 30k to 40k"', '"MBM Income ($) - 350k to 500k"', '"MBM Income ($) - 50k to 75k"', '"MBM Race - Hispanic (All)"', '"MBM Race - Other"', '"MBM Race - Asian-American"', 
                    '"MBM Race - African-American"', '"MBM Gender - Male"', '"MBM Gender - Female"', '"MBM Education - Grad school"', '"MBM Education - College"', '"MBM Education - High School"']
    with open(path,'r',encoding='utf-8') as f: # read in full file data
            data = f.read()
            data = (','.join(headers) + '\n' + data) # prepend headers
    with open(path,'w',encoding='utf-8') as f:
        f.write(u'\ufeff'+ data) # prepend byte order mark

# returns sql query string for generating a CSV of the full dataset
def full_dataset_sql(filename):
    return """SELECT a.idArticle, CONCAT(date(a.datetime), '_', LPAD(a.n, 3, '0')) as alt_id, a.datetime, a.source, IFNULL(sb.mbfc_bias,''), IFNULL(sb.mbfc_score,''), IFNULL(sb.mbfc_z,''), IFNULL(sb.mbfc_factual_reporting,''), IFNULL(sb.allsides_bias,''), IFNULL(sb.allsides_score,''), IFNULL(sb.allsides_z,''), IFNULL(sb.allsides_confidence,''), 
            IFNULL(sb.allsides_agree,''), IFNULL(sb.allsides_disagree,''), IFNULL(sb.mbm_score,''), IFNULL(sb.mbm_z,''), a.url, a.title, a.author, IFNULL(a.relevancy_score,''), IFNULL(a.score,''), IFNULL(a.magnitude,''), IFNULL(i.top_entity,''), IFNULL(i.top_entity_score,''), k.keywords, 
            IFNULL(sa.similarBefore,''), IFNULL(sa.similarAfter,''), IFNULL(a.fb_reactions_initial,''), IFNULL(a.fb_reactions_d1,''), IFNULL(a.fb_reactions_d7,''), IFNULL(a.fb_comments_initial,''), IFNULL(a.fb_comments_d1,''), IFNULL(a.fb_comments_d7,''), IFNULL(a.fb_shares_initial,''), 
            IFNULL(a.fb_shares_d1,''), IFNULL(a.fb_shares_d7,''), IFNULL(a.fb_comment_plugin_initial,''), IFNULL(a.fb_comment_plugin_d1,''), IFNULL(a.fb_comment_plugin_d7,''), IFNULL(a.tw_tweets_initial,''), IFNULL(a.tw_tweets_d1,''), IFNULL(a.tw_tweets_d7,''), 
            IFNULL(a.tw_favorites_initial,''), IFNULL(a.tw_favorites_d1,''), IFNULL(a.tw_favorites_d7,''), IFNULL(a.tw_retweets_initial,''), IFNULL(a.tw_retweets_d1,''), IFNULL(a.tw_retweets_d7,''), IFNULL(a.tw_top_favorites_initial,''), IFNULL(a.tw_top_favorites_d1,''), 
            IFNULL(a.tw_top_favorites_d7,''), IFNULL(a.tw_top_retweets_initial,''), IFNULL(a.tw_top_retweets_d1,''), IFNULL(a.tw_top_retweets_d7,''), IFNULL(a.rdt_posts_initial,''), IFNULL(a.rdt_posts_d1,''), IFNULL(a.rdt_posts_d7,''), IFNULL(a.rdt_total_comments_initial,''), 
            IFNULL(a.rdt_total_comments_d1,''), IFNULL(a.rdt_total_comments_d7,''), IFNULL(a.rdt_total_scores_initial,''), IFNULL(a.rdt_total_scores_d1,''), IFNULL(a.rdt_total_scores_d7,''), IFNULL(a.rdt_top_comments_initial,''), IFNULL(a.rdt_top_comments_d1,''), 
            IFNULL(a.rdt_top_comments_d7,''), IFNULL(a.rdt_top_score_initial,''), IFNULL(a.rdt_top_score_d1,''), IFNULL(a.rdt_top_score_d7,''), IFNULL(a.rdt_top_ratio_initial,''), IFNULL(a.rdt_top_ratio_d1,''), IFNULL(a.rdt_top_ratio_d7,''), IFNULL(a.rdt_avg_ratio_initial,''), 
            IFNULL(a.rdt_avg_ratio_d1,''), IFNULL(a.rdt_avg_ratio_d7,''), IFNULL(sb.mbm_pol_align_very_conservative,''), IFNULL(sb.mbm_pol_align_very_liberal,''), IFNULL(sb.mbm_pol_align_moderate,''), IFNULL(sb.mbm_pol_align_liberal,''), 
            IFNULL(sb.mbm_pol_align_conservative,''), IFNULL(sb.mbm_pol_engage_moderate,''), IFNULL(sb.mbm_pol_engage_liberal,''), IFNULL(sb.mbm_pol_engage_conservative,''), IFNULL(sb.mbm_age_young_2,''), IFNULL(sb.mbm_age_mid_aged_1,''), IFNULL(sb.mbm_age_mid_aged_2,''), 
            IFNULL(sb.mbm_age_adolescent,''), IFNULL(sb.mbm_age_old_2,''), IFNULL(sb.mbm_age_old_1,''), IFNULL(sb.mbm_age_young_1,''), IFNULL(sb.mbm_income_250k_to_350k,''), IFNULL(sb.mbm_income_75k_to_100k,''), IFNULL(sb.mbm_income_over_500k,''), IFNULL(sb.mbm_income_125k_to_150k,''), 
            IFNULL(sb.mbm_income_40k_to_50k,''), IFNULL(sb.mbm_income_150k_to_250k,''), IFNULL(sb.mbm_income_100k_to_125k,''), IFNULL(sb.mbm_income_30k_to_40k,''), IFNULL(sb.mbm_income_350k_to_500k,''), IFNULL(sb.mbm_income_50k_to_75k,''), IFNULL(sb.mbm_race_hispanic_all,''), 
            IFNULL(sb.mbm_race_other,''), IFNULL(sb.mbm_race_asian_american,''), IFNULL(sb.mbm_race_african_american,''), IFNULL(sb.mbm_gen_male,''), IFNULL(sb.mbm_gen_female,''), IFNULL(sb.mbm_edu_grad_school,''), IFNULL(sb.mbm_edu_college,''), IFNULL(sb.mbm_edu_high_school,'')
            INTO OUTFILE '/var/www/html/scotusapp/webapp/%s.csv' 
            FIELDS TERMINATED BY ',' ENCLOSED BY '\\\"' 
            ESCAPED BY '\"' 
            LINES TERMINATED BY '\\r\\n'
            FROM article a
                NATURAL JOIN
                    (SELECT idArticle, GROUP_CONCAT(keyword ORDER BY keyword ASC) as keywords from keyword_instances NATURAL JOIN article_keywords GROUP BY idArticle) k
                LEFT JOIN
                    (SELECT ii.idArticle,ii.entity as top_entity,imax.top_entity_score
                        FROM (SELECT idArticle,entity,score FROM image NATURAL JOIN entity_instances NATURAL JOIN image_entities) ii
                        INNER JOIN
                        (SELECT idArticle,entity,MAX(score) as top_entity_score FROM image NATURAL JOIN entity_instances NATURAL JOIN image_entities GROUP BY idArticle) imax
                        ON ii.idArticle=imax.idArticle AND ii.entity=imax.entity AND ii.score=imax.top_entity_score) i 
                    ON a.idArticle=i.idArticle
                LEFT JOIN (
                    SELECT source_bias.*, ROUND((allsides_score - allsides_mean)/allsides_sd,2) as allsides_z, ROUND((mbfc_score - mbfc_mean)/mbfc_sd,2) as mbfc_z, ROUND((mbm_score - mbm_mean)/mbm_sd,2) as mbm_z 
                    FROM source_bias
                    CROSS JOIN (
                        SELECT AVG(allsides_score) as allsides_mean, AVG(mbfc_score) as mbfc_mean, AVG(mbm_score) as mbm_mean, 
                        STD(allsides_score) as allsides_sd, STD(mbfc_score) as mbfc_sd, STD(mbm_score) as mbm_sd
                        FROM source_bias
                        WHERE source in (SELECT DISTINCT source FROM article)
                        ) agg
                    WHERE source in (SELECT DISTINCT source FROM article)
                ) sb
                ON a.source = sb.source
                LEFT JOIN
                    (
                        SELECT sa_raw.idArticle, GROUP_CONCAT(CASE WHEN idArticle_datetime >= otherArticle_datetime THEN CONCAT(otherArticle,':',similarity) ELSE null END ORDER BY similarity DESC, otherArticle DESC) as similarBefore,
                        GROUP_CONCAT(CASE WHEN idArticle_datetime < otherArticle_datetime THEN CONCAT(otherArticle,':',similarity) ELSE null END ORDER BY similarity DESC, otherArticle DESC) as similarAfter
                        FROM 
                        (SELECT article1 as idArticle,article2 as otherArticle,similarity FROM similar_articles
                        UNION ALL
                        SELECT article2 as idArticle,article1 as otherArticle,similarity FROM similar_articles) sa_raw
                        INNER JOIN
                        (SELECT idArticle, datetime as idArticle_datetime FROM article) ia
                        ON ia.idArticle=sa_raw.idArticle
                        INNER JOIN
                        (SELECT idArticle, datetime as otherArticle_datetime FROM article) oa
                        ON oa.idArticle=sa_raw.otherArticle
                        GROUP BY sa_raw.idArticle
                    ) sa 
                    ON a.idArticle=sa.idArticle
                ORDER BY a.idArticle DESC""" % filename

# authenticate Google Drive account
# returns Google Drive to work with
def GDrive_auth(configpath,credpath):
    try:
        gauth = GoogleAuth()
        gauth.DEFAULT_SETTINGS['client_config_file'] = configpath
        gauth.LoadCredentialsFile(credpath)
        if gauth.credentials is None:
            gauth.LocalWebserverAuth() # perform web browser authentication (this shouldn't occur as long as we have a credentials file)
        elif gauth.access_token_expired: # refresh access token using refresh token
            gauth.Refresh()
        else:
            gauth.Authorize()
        gauth.SaveCredentialsFile(credpath) # update any credentials changes
        gdrive = GoogleDrive(gauth)
    except Exception as e:
        print("Google Drive failed to authenticate:",e)
        gdrive = None
    return gdrive