# Supreme Court Coverage & Analysis Application (SCOTUSApp)

SCOTUSApp is an academic research web application commissioned by Dr. Michael Zilis and Dr. Justin Wedeking of the University of Kentucky Political Science Department. SCOTUSApp finds, stores, and analyzes articles pertaining to the United States Supreme Court. It is currently developed by Evan Cole.

The application consists of three major components:

### Article Collector (/articleCollector/) [Python + helper shell scripts]
The Article Collector (written in Python, with helper shell scripts) is the driving force of the project, handling the collection, storage, and analysis of articles. Articles are gathered via custom Google Alerts RSS Feeds, feeds generated by NewsAPI, and SCOTUS-specific news pages from various major news sources (referred to as Topic Sites), and scraped using a combination of custom scrapers (for certain sites) and the Newspaper Python library (for everything else) . After an article's text and metadata is collected, it is tested against our text classification system for relevancy - if deemed relevant to the SCOTUS (not a foreign or lower court - for example, the Indian Supreme Court, Kentucky Supreme Court, or a federal circuit court), then the article is entered into our database and analyzed. Analysis metrics include text sentiment scores, image entities (and appropriate scores), and social media popularity across three different platforms (Facebook, Twitter, Reddit) and three different time periods (initial entry into our database, 1 day after publication date, and one week after publication date). Data courtesy of AllSides and Media Bias Fact Check connect our articles to source bias data for examining the political leanings of various news outlets.

The Article Collector is entirely automated, running throughout the day in order to gather articles as they are released. In order to provide the best data, an alerts system has been developed to notify the administrators of any scraping issues as they occur.

### Database [MySQL]
All of the project's data is stored on a MySQL database. Data collection began in October 2017 and is ongoing.

### Web Application (/webapp/) [PHP, Javascript, JQuery / uses Bootstrap elements] 
The Web Application allows for users to search, view, and download the project's data. Downloads come in the form of a .zip containing a .CSV with rows of article data, and .txt files containing article texts. Users must be registered and approved by administrators before use of the application can begin.

For understanding the other directories in this repo:

### Text Files (/txtfiles/)
.txt files containing full article texts are stored here. Files are named according to an article's ID.

### Images (/images/)
As described above, this is where any scraped article images go. Up until recently, images were titled based on the Image ID in the database - now, they are titled based on the Article ID they belong to (older images have not been renamed).

### Documentation (/docs/)
A collection of text files and PDF manuals on how to run and manage the application.

### Install (/install/)
Files relating to the installation and maintenance of the application, such as credentials and backup/cronjob shell scripts. Most of the files in this directory aren't uploaded to Github as they contain sensitive information.

### Backups (/backups/)
Files pertaining to the backup of project data. As we have a private automated backup system, not everything here is included in this repo.

### Miscellaneous Scripts (/misc/)
Files and scripts for completing various housekeeping tasks related to the project. They may be incomplete or rough around the edges.